Robert W. Floyd: Syntactic Analysis and Operator Precedence.

Summarized by Arno Bastenhof

We adopt the ISO-standard for (E)BNF, augmented by the following conventions:

    x, y, z             arbitrary strings of symbols, possibly empty
    u, u_1, u_2, ...    non-terminal symbols
    =*                  reflexive-transitive closure of =

The last notation may seem redundant, owing to the ISO standard's use of "="
(suggesting symmetry) for specifying productions. The following (left-recursive)
grammar in BNF with start symbol expr will recur for purposes of illustration:

    expr   = term   | expr, "+", term   ;
    term   = factor | term, "*", factor ;
    factor = "ID"   | "(", expr, ")"    ;

A simple expression language is thus defined with binary operators + and *,
assigning the latter higher precedence. The current article shows how to extract
such precedence relations from a subclass of phrase structure grammars and uses
them for a bottom-up parsing procedure, characterized by the 'reducing' of a
sentential form through the repeated inverse application of productions. The ex-
pansion of a production is recognized in a sentential form as a 'simple phrase':

    y is a (simple) phrase of a sentential form x, y, z iff there exists a
    sentential form x, u, z with u =* y; (u = y;).

If an efficient means is known for recognizing simple phrases, such can be used
as the basis for a non-deterministic parsing procedure, based on the observation
that their repeated reduction is guaranteed to halt (signaling success if the
start symbol was reached). Note 'non-deterministic' means we may be required to
back up on a previous choice for the next simple phrase to reduce if we are to
find all possible derivation trees (even if there's only one.)

The program carried out by Floyd is roughly as sketched above. As is typical for
such efforts, he restricts to a subclass of all phrase structure grammars. In
particular, with the problem of determining their ambiguity undecidable in ge-
neral, the challenge is usually perceived as discerning a wide enough subclass
that satisfies this property while admitting refinements to the above parsing
method that render it deterministic, obviating the need to back up for finding
a sentence's now unique derivation tree. The restrictions chosen by Floyd,
however, sugest a natural alternative to simple phrases.

Returning to our sample grammar, we observe that in none of its productions do
two non-terminals occur adjacent. This observation can be shown to extend to all
sentential forms, and we speak in general of an operator grammar. Note that, for
such grammars, a simple phrase consists of either a single non-terminal, or else
contains at least one terminal symbol (ignoring empty productions). By ignoring
the former, we naturally arrive at the notion of a prime phrase:

    A phrase is prime iff it contains at least one terminal character and no
    prime phrases beside itself.

The following are examples of prime phrases, based on our previous grammar:

    expr, "+', term     factor, "+", term
    term, "+', term     factor, "+", factor

As seen, a prime phrase need not also be a simple phrase. In particular, it may
itself contain simple phrases comprised of only a non-terminal. However, by re-
peatedly reducing the latter, the result is always a prime phrase that is simple

Floyd's main result is the definition of several relations on a grammar's
terminals that allow to efficiently recognize prime phrases:

(1) "a" = "b" iff for some x, y, u, u_1, (e.g., "(" = ")")

        u = x, "a", "b", y;
    or  u = x, "a", u_1, "b", y;

(2) "a" > "b" iff for some x, y, u, u_1, (e.g., "+" < "*")

        u = x, u_1, "b", y;
    and "a" is a Rightmost Terminal Character of some Derivative (RTCD) of u_1

(3) "a" < "b" iff for some x, y, u, u_1, (e.g., "*" > "+")

        u =  x, "a", u_1, y;
    and "b" is a Leftmost Terminal Character of some Derivative (LTCD) of u_1

Before showing how these relations aid in the identification of prime phrases,
we first describe their effective computation. First, by definition, = is read
off from individual productions. E.g.,

    factor = "(", expr, ")";

implies "(" = ")". In contrast, the definitions of < and > require identifying
the Left- and Rightmost Terminal Characters of each non-terminal's Derivatives:

    non-terminal    LTCD                RTCD
    -----------------------------------------------------------
    expr            "ID", "(", "+"      "ID", ")", "+", "*"
    term            "ID", "(", "*"      "ID", ")", "*"
    factor          "ID", "("           "ID", ")"

For example, that both "+" and "*" are RTCD's for expr follows from

    expr =  expr, "+", term;
    expr =* expr, "+", term, "*", factor;

Now, we may conclude, e.g., that "+" > "+", given that

        expr = expr, "+", term;
    and    "+" is a Rightmost Terminal Character of expr's Derivatives

In fact, we find that at most one precedence relation holds of any two terminals
in our case. As such, we can exhaustively describe all precedence relations
using an n x n matrix for n the number of terminals:

          (    ID   *    +    )
       ------------------------
    )  |  >    >    >
    ID |  >    >    >
    *  |  <    <    >    >    >
    +  |  <    <    <    >    >
    (  |  <    <    <    <    =

Operator grammars satisfying the above property are designated precedence
grammars. We shall focus exclusively on them below, with matrices such as above
used for performing lookups in what essentially becomes table-driven parsing.

It can be shown that a string x with terminals "a_1", ..., "a_n" is prime rela-
tive to "b", x, "c" iff "b" < "a_1", "a_i" = "a_(i+1)" for 1 <= i <= n-1 and
"a_n" > "c" While generalizable to the absence of either "b" or "c", we may w/h
loss of generality assume a fresh start symbol s' and terminals "|-", "-|" s.t.

    s' = "|-", s, "-|";

for s the old start symbol. Given a proposed sentential form "|-", x, "-|", we
build a skeletal derivation bottom-up by iteratively replacing a prime phrase
with a fresh non-terminal character u_i (for i = 1, 2, ...). E.g.,

        "|-", "(", "ID", "+", "ID", ")", "*", "ID", "-|"
    =*  "|-", "(",  u_1, "+", "ID", ")", "*", "ID', "-|"
    =*  "|-", "(",  u_1, "+",  u_2, ")", "*", "ID", "-|"
    =*  "|-", "(",             u_3, ")", "*", "ID", "-|"
    =*  "|-",                       u_4, "*", "ID", "-|"
    =*  "|-",                       u_4, "*",  u_5, "-|"
    =*  "|-",                                  u_6, "-|";

The problem of determining whether the original string was a sentential form is
now reduced to solving for u_1, ..., u_6,

    u_1 =* "ID";    u_3 =* u_1, "+", u_2;   u_5 =* "ID";
    u_2 =* "ID";    u_4 =* "(", u_3, ")";   u_6 =* u_4, "*", u_5;

s.t. expr =* u_6;. We have the following solution, based on the productions
expr = term, term = factor and factor = ID:

    u_1 := expr    u_3 := expr    u_5 := factor
    u_2 := term    u_4 := term    u_6 := term

It can be shown that precedence grammars are non-ambiguous, and that always
restricting to the reduction of the leftmost prime phrase suffices for deciding
derivability without any need for backing up.

We now discuss several improvements. First, note a precedence matrix takes space
quadratic in the number of terminals. In most cases, we can compute precedence
functions f and g mapping terminals to numbers s.t.

    "a" < "b"    implies    f("a") < g("b")
    "a" = "b"    implies    f("a") = g("b")
    "a" > "b"    implies    f("a") > g("b")

offering a linear alternative to the precedence matrix. E.g.,

        ")"    "ID"     "*"     "-"     ")"
    -------------------------------------------
    f     5     5       5       3       1
    g     1     6       4       2       6

Next, consider the extension of our previous grammar with unary addition:

    expr    = term    | expr, "+", term;
    term    = factor  | term, "*", factor;
    factor  = primary | "+", factor;
    primary = "ID"    | "(", expr, ")";

Since now both "*" > "+" and "*" < "+", this is no longer a precedence grammar.
However, the latter property is restored if we represent binary and unary "+"
by different symbols. Digging deeper, we find that the sets of symbols that
may directly precede either usages of "+" are disjoint, and hence offer an
alternative means for disambiguation, as opposed to using distinct symbols.
